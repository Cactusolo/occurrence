<?xml version="1.0" encoding="utf-8"?>
<!-- ~ Copyright 2012 Global Biodiversity Information Facility (GBIF) ~ ~
  Licensed under the Apache License, Version 2.0 (the "License"); ~ you may
  not use this file except in compliance with the License. ~ You may obtain
  a copy of the License at ~ ~ http://www.apache.org/licenses/LICENSE-2.0 ~
  ~ Unless required by applicable law or agreed to in writing, software ~ distributed
  under the License is distributed on an "AS IS" BASIS, ~ WITHOUT WARRANTIES
  OR CONDITIONS OF ANY KIND, either express or implied. ~ See the License for
  the specific language governing permissions and ~ limitations under the License. -->
<workflow-app name="occurrence.download" xmlns="uri:oozie:workflow:0.2">

  <start to="big_download" />

  <!-- unused, right now we only do big scans


  <action name="occurrence_count">
    <java>
      <job-tracker>${hadoop.jobtracker}</job-tracker>
      <name-node>${hdfs.namenode}</name-node>
      <job-xml>conf/hive-default.xml</job-xml>
      <main-class>org.gbif.occurrence.download.oozie.OccurrenceCount</main-class>
      <arg>${solr_query}</arg>
      <arg>${wf:id()}</arg>
      <capture-output/>
    </java>

    <ok to="make_query_decision" />
    <error to="kill" />
  </action>

  <decision name="make_query_decision">
    <switch>
      <!-- it's a small download -- >
      <case to="small_download">
        ${wf:actionData('occurrence_count')['is_small_download']}
      </case>
      <default to="big_download" />
    </switch>
  </decision>

  <action name="small_download">
    <java>
      <job-tracker>${hadoop.jobtracker}</job-tracker>
      <name-node>${hdfs.namenode}</name-node>
      <job-xml>conf/hive-default.xml</job-xml>
      <main-class>org.gbif.occurrence.download.file.DownloadTableBuilder</main-class>
      <arg>${query_result_table}</arg>
      <arg>${citation_table}</arg>
      <arg>${solr_query}</arg>
      <arg>${hdfs.namenode}</arg>
      <arg>${hive.hdfs.out}</arg>
      <arg>${wf:id()}</arg>
    </java>

    <ok to="copy_and_zip" />
    <error to="failure_clean_hdfs_dirs" />
  </action>
  -->

  <fork name="big_download">
    <path start="query_interpreted" />
    <path start="query_verbatim" />
  </fork>

  <action name="query_interpreted">
    <hive xmlns="uri:oozie:hive-action:0.2">
      <job-tracker>${hadoop.jobtracker}</job-tracker>
      <name-node>${hdfs.namenode}</name-node>
      <job-xml>conf/hive-default.xml</job-xml>
      <script>hive-scripts/query-interpreted.q</script>

      <param>occurrence_record=${occurrence.environment}.occurrence_hdfs</param>
      <param>select=${select_interpreted}</param>
      <param>query=${query}</param>
      <param>query_result=${query_result_table}</param>
    </hive>

    <ok to="citation" />
    <error to="failure_drop_table" />
  </action>

  <action name="query_verbatim">
    <hive xmlns="uri:oozie:hive-action:0.2">
      <job-tracker>${hadoop.jobtracker}</job-tracker>
      <name-node>${hdfs.namenode}</name-node>
      <job-xml>conf/hive-default.xml</job-xml>
      <script>hive-scripts/query-verbatim.q</script>

      <param>occurrence_record=${occurrence.environment}.occurrence_hdfs</param>
      <param>select=${select_verbatim}</param>
      <param>query=${query}</param>
      <param>query_result=${query_result_table}_verbatim</param>
    </hive>

    <ok to="bundling" />
    <error to="failure_drop_table" />
  </action>

  <action name="citation">
    <hive xmlns="uri:oozie:hive-action:0.2">
      <job-tracker>${hadoop.jobtracker}</job-tracker>
      <name-node>${hdfs.namenode}</name-node>
      <job-xml>conf/hive-default.xml</job-xml>
      <script>hive-scripts/citation.q</script>

      <param>query_result=${query_result_table}_interpreted</param>
      <param>citation=${citation_table}</param>
    </hive>

    <ok to="bundling" />
    <error to="failure_drop_table" />
  </action>

  <join name="bundling" to="copy_and_zip"/>

  <action name="copy_and_zip">
    <java>
      <job-tracker>${hadoop.jobtracker}</job-tracker>
      <name-node>${hdfs.namenode}</name-node>
      <job-xml>conf/hive-default.xml</job-xml>
      <configuration>
        <property>
          <name>mapred.queue.name</name>
          <value>default</value>
        </property>
      </configuration>

      <main-class>org.gbif.occurrence.download.oozie.ArchiveBuilder</main-class>
      <arg>${hdfs.namenode}</arg>
      <arg>${hive.hdfs.out}</arg>
      <arg>${query_result_table}_interpreted</arg>
      <arg>${query_result_table}_verbatim</arg>
      <arg>${citation_table}</arg>
      <arg>/mnt/auto/${occurrence.environment}/occurrence-download</arg>
      <arg>${wf:id()}</arg>
      <arg>${gbif_user}</arg>
      <arg>${gbif_filter}</arg>
      <arg>${download_link}</arg>
      <arg>${registry.ws.url}</arg>
      <arg>false</arg>
    </java>

    <ok to="cleaning_decision" />
    <error to="failure_cleaning_decision" />
  </action>

  <decision name="cleaning_decision">
    <switch>
      <!-- it's a small download the delete the occurrence data and citations file -->
      <case to="clean_hdfs_dirs">
        ${wf:actionData('occurrence_count')['is_small_download']}
      </case>
      <default to="drop_table" />
    </switch>
  </decision>

  <decision name="failure_cleaning_decision">
    <switch>
      <!-- it's a small download the delete the occurrence data and citations file -->
      <case to="failure_clean_hdfs_dirs">
       false
      </case>
      <default to="failure_drop_table" />
    </switch>
  </decision>

  <action name="drop_table">
    <hive xmlns="uri:oozie:hive-action:0.2">
      <job-tracker>${hadoop.jobtracker}</job-tracker>
      <name-node>${hdfs.namenode}</name-node>
      <job-xml>conf/hive-default.xml</job-xml>
      <script>hive-scripts/drop_table.q</script>

      <param>query_result=${query_result_table}</param>
      <param>citation=${citation_table}</param>
    </hive>

    <ok to="end" />
    <error to="kill" />
  </action>


  <action name="failure_drop_table">
    <hive xmlns="uri:oozie:hive-action:0.2">
      <job-tracker>${hadoop.jobtracker}</job-tracker>
      <name-node>${hdfs.namenode}</name-node>
      <job-xml>conf/hive-default.xml</job-xml>
      <script>hive-scripts/drop_table.q</script>

      <param>query_result=${query_result_table}</param>
      <param>citation=${citation_table}</param>
    </hive>

    <ok to="kill" />
    <error to="kill" />
  </action>

  <action name="clean_hdfs_dirs">
    <fs>
      <delete path='${hdfs.namenode}/${hive.hdfs.out}/${query_result_table}'/>
      <delete path='${hdfs.namenode}/${hive.hdfs.out}/${citation_table}'/>
    </fs>
    <ok to="end" />
    <error to="kill" />
  </action>

  <action name="failure_clean_hdfs_dirs">
    <fs>
      <delete path='${hdfs.namenode}/${hive.hdfs.out}/${query_result_table}'/>
      <delete path='${hdfs.namenode}/${hive.hdfs.out}/${citation_table}'/>
    </fs>
    <ok to="kill" />
    <error to="kill" />
  </action>

  <kill name="kill">
    <message>Occurrence download failed:[${wf:errorMessage(wf:lastErrorNode())}]</message>
  </kill>

  <end name="end" />

</workflow-app>
